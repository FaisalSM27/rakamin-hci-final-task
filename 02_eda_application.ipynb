{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49de64eb-5db9-4218-b10f-949cd2107a04",
   "metadata": {},
   "source": [
    "# üè¶ Home Credit Default Risk \n",
    "## Notebook 2: EDA & Preprocessing\n",
    "**Nama:** [Faisal Soultan Muhammad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad373bdf-640b-47f4-96fc-4295383375c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Notebook 2: EDA Started\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"‚úÖ Notebook 2: EDA Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe9b98-4ecd-4aff-895a-5ff75a6b2e51",
   "metadata": {},
   "source": [
    "## 1. Load Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031726bd-f119-49d6-b693-5215fe192f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATA LOADED:\n",
      "‚Ä¢ Shape: (307511, 122)\n",
      "‚Ä¢ Target distribution: {0: 282686, 1: 24825}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load data from Notebook 1\n",
    "with open('app_train.pkl', 'rb') as f:\n",
    "    app_train = pickle.load(f)\n",
    "\n",
    "with open('metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(\"üìä DATA LOADED:\")\n",
    "print(f\"‚Ä¢ Shape: {app_train.shape}\")\n",
    "print(f\"‚Ä¢ Target distribution: {metadata['target_counts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b62c24-acc9-40e6-97f1-fb2e97bc69cb",
   "metadata": {},
   "source": [
    "## 2. EDA Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886bbdce-1f2e-4733-b948-223d6fefa6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç EDA ANALYSIS\n",
      "============================================================\n",
      "1. Target mean: 0.081 (8.1% default)\n",
      "\n",
      "2. Missing values:\n",
      "   ‚Ä¢ Columns >50% missing: 41\n",
      "   ‚Ä¢ Will drop: ['OWN_CAR_AGE', 'EXT_SOURCE_1', 'APARTMENTS_AVG']...\n",
      "\n",
      "3. Data types:\n",
      "   ‚Ä¢ Numerical: 106 columns\n",
      "   ‚Ä¢ Categorical: 16 columns\n"
     ]
    }
   ],
   "source": [
    "def fast_eda(df, target_col='TARGET'):\n",
    "    print(\"üîç EDA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Target analysis\n",
    "    if target_col in df.columns:\n",
    "        target_mean = df[target_col].mean()\n",
    "        print(f\"1. Target mean: {target_mean:.3f} ({target_mean*100:.1f}% default)\")\n",
    "        results['target_mean'] = target_mean\n",
    "    \n",
    "    # 2. Missing values quick fix\n",
    "    missing_pct = df.isnull().mean() * 100\n",
    "    high_missing = missing_pct[missing_pct > 50].index.tolist()\n",
    "    \n",
    "    print(f\"\\n2. Missing values:\")\n",
    "    print(f\"   ‚Ä¢ Columns >50% missing: {len(high_missing)}\")\n",
    "    if high_missing:\n",
    "        print(f\"   ‚Ä¢ Will drop: {high_missing[:3]}...\" if len(high_missing)>3 else f\"   ‚Ä¢ Will drop: {high_missing}\")\n",
    "    \n",
    "    # 3. Column types\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\n3. Data types:\")\n",
    "    print(f\"   ‚Ä¢ Numerical: {len(num_cols)} columns\")\n",
    "    print(f\"   ‚Ä¢ Categorical: {len(cat_cols)} columns\")\n",
    "    \n",
    "    results.update({\n",
    "        'high_missing': high_missing,\n",
    "        'num_cols': num_cols,\n",
    "        'cat_cols': cat_cols\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run EDA\n",
    "eda_results = fast_eda(app_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816b7a3-a37a-464d-a2af-ae85470905e8",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6f16b4-c613-4f95-beb6-1e3d2f72fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üßπ DATA CLEANING\n",
      "============================================================\n",
      "‚úÖ Dropped 41 columns with >50% missing\n",
      "‚úÖ Filled missing numerical values with median\n",
      "‚úÖ Filled missing categorical values with mode\n",
      "‚úÖ Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üßπ DATA CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create copy for processing\n",
    "df = app_train.copy()\n",
    "\n",
    "# 1. Drop columns with >50% missing\n",
    "cols_to_drop = eda_results['high_missing']\n",
    "if cols_to_drop:\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(f\"‚úÖ Dropped {len(cols_to_drop)} columns with >50% missing\")\n",
    "\n",
    "# 2. Fill missing numerical values with median\n",
    "num_cols = [col for col in df.select_dtypes(include=[np.number]).columns if col != 'TARGET']\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "print(f\"‚úÖ Filled missing numerical values with median\")\n",
    "\n",
    "# 3. Fill missing categorical values with mode\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "print(f\"‚úÖ Filled missing categorical values with mode\")\n",
    "\n",
    "# Check remaining missing\n",
    "remaining_missing = df.isnull().sum().sum()\n",
    "print(f\"‚úÖ Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366733a2-4768-43dd-a5c5-d673da69c55d",
   "metadata": {},
   "source": [
    "## 4. Categorical Encoding - SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326a6721-d69a-43c6-b84d-33bea35f71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üî§ CATEGORICAL ENCODING\n",
      "============================================================\n",
      "‚Ä¢ Encoded: NAME_CONTRACT_TYPE (2 unique values)\n",
      "‚Ä¢ Encoded: CODE_GENDER (3 unique values)\n",
      "‚Ä¢ Encoded: FLAG_OWN_CAR (2 unique values)\n",
      "‚Ä¢ Encoded: FLAG_OWN_REALTY (2 unique values)\n",
      "‚Ä¢ Encoded: NAME_TYPE_SUITE (7 unique values)\n",
      "‚Ä¢ Encoded: NAME_INCOME_TYPE (8 unique values)\n",
      "‚Ä¢ Encoded: NAME_EDUCATION_TYPE (5 unique values)\n",
      "‚Ä¢ Encoded: NAME_FAMILY_STATUS (6 unique values)\n",
      "‚Ä¢ Encoded: NAME_HOUSING_TYPE (6 unique values)\n",
      "‚Ä¢ Encoded: OCCUPATION_TYPE (18 unique values)\n",
      "‚Ä¢ Encoded: WEEKDAY_APPR_PROCESS_START (7 unique values)\n",
      "‚Ä¢ Encoded: ORGANIZATION_TYPE (58 unique values)\n",
      "‚Ä¢ Encoded: EMERGENCYSTATE_MODE (2 unique values)\n",
      "\n",
      "‚úÖ Encoded 13 categorical columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî§ CATEGORICAL ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Label encoding for all categorical columns (fastest approach)\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"‚Ä¢ Encoded: {col} ({df[col].nunique()} unique values)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Encoded {len(cat_cols)} categorical columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d059485-7d31-4431-8362-3fe5911d03df",
   "metadata": {},
   "source": [
    "## 5. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0999af94-e3c2-48e2-b378-4d47a1ff7a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚öñÔ∏è HANDLING CLASS IMBALANCE\n",
      "============================================================\n",
      "Class weights computed:\n",
      "‚Ä¢ Class 0 (Non-Default): weight = 0.54\n",
      "‚Ä¢ Class 1 (Default): weight = 6.19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öñÔ∏è HANDLING CLASS IMBALANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use class weights \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y = df['TARGET']\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"Class weights computed:\")\n",
    "print(f\"‚Ä¢ Class 0 (Non-Default): weight = {weight_dict[0]:.2f}\")\n",
    "print(f\"‚Ä¢ Class 1 (Default): weight = {weight_dict[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a6b68-abf0-4884-adda-22c477070127",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba2a547-73ce-4fae-953e-6d5b2ecae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ QUICK FEATURE SELECTION\n",
      "============================================================\n",
      "Selected top 30 features by correlation with target:\n",
      " 1. EXT_SOURCE_2                   | correlation: 0.1603\n",
      " 2. EXT_SOURCE_3                   | correlation: 0.1559\n",
      " 3. DAYS_BIRTH                     | correlation: 0.0782\n",
      " 4. REGION_RATING_CLIENT_W_CITY    | correlation: 0.0609\n",
      " 5. REGION_RATING_CLIENT           | correlation: 0.0589\n",
      " 6. DAYS_LAST_PHONE_CHANGE         | correlation: 0.0552\n",
      " 7. NAME_EDUCATION_TYPE            | correlation: 0.0547\n",
      " 8. CODE_GENDER                    | correlation: 0.0547\n",
      " 9. DAYS_ID_PUBLISH                | correlation: 0.0515\n",
      "10. REG_CITY_NOT_WORK_CITY         | correlation: 0.0510\n",
      "   ... and 20 more\n",
      "\n",
      "‚úÖ Final dataset shape: X=(307511, 30), y=(307511,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ QUICK FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select features with correlation to target\n",
    "correlations = {}\n",
    "for col in df.columns:\n",
    "    if col != 'TARGET' and df[col].dtype in [np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]:\n",
    "        corr = df[col].corr(df['TARGET'])\n",
    "        correlations[col] = abs(corr)\n",
    "\n",
    "# Top 30 features by correlation\n",
    "top_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "top_feature_names = [f[0] for f in top_features]\n",
    "\n",
    "print(f\"Selected top 30 features by correlation with target:\")\n",
    "for i, (feat, corr) in enumerate(top_features[:10], 1):\n",
    "    print(f\"{i:2}. {feat:30} | correlation: {corr:.4f}\")\n",
    "print(\"   ... and 20 more\")\n",
    "\n",
    "# Create final dataset with selected features\n",
    "X = df[top_feature_names]\n",
    "y = df['TARGET']\n",
    "\n",
    "print(f\"\\n‚úÖ Final dataset shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e56c5-364f-4f44-b31d-9a0d33ff2cd7",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec200c19-929d-493c-8e7f-a94610ca4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä TRAIN/TEST SPLIT\n",
      "============================================================\n",
      "Training set: 246,008 samples\n",
      "Testing set: 61,503 samples\n",
      "Feature count: 30\n",
      "‚úÖ Features scaled with StandardScaler\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Feature count: {X_train.shape[1]}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled with StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffb0c6-8344-474d-ab3a-0d6c4709675a",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b42a5766-e49e-46fd-9aa1-7184890da402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üíæ SAVING PROCESSED DATA\n",
      "============================================================\n",
      "‚úÖ Saved: processed_data.pkl\n",
      "‚úÖ Saved: top_features.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAVING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save processed data\n",
    "processed_data = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': top_feature_names,\n",
    "    'class_weights': weight_dict,\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"‚úÖ Saved: processed_data.pkl\")\n",
    "\n",
    "# Save feature info\n",
    "feature_info = pd.DataFrame({\n",
    "    'feature': top_feature_names,\n",
    "    'correlation_with_target': [correlations[f] for f in top_feature_names]\n",
    "})\n",
    "feature_info.to_csv('top_features.csv', index=False)\n",
    "print(\"‚úÖ Saved: top_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafb8e8-a10e-485f-baaa-1ad69b8714f8",
   "metadata": {},
   "source": [
    "## üéØ READY FOR MODELLING!\n",
    "\n",
    "**Notebook 2 selesai**\n",
    "\n",
    "**Lanjut ke Notebook 3:** `03_model_training.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d71559-282c-49d9-a3a1-eb1e4e5ac04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ NOTEBOOK 2 COMPLETED!\n",
      "============================================================\n",
      "\n",
      "‚úÖ Data cleaned and preprocessed\n",
      "‚úÖ Categorical variables encoded\n",
      "‚úÖ Feature selection completed\n",
      "‚úÖ Train/test split created\n",
      "\n",
      "‚û°Ô∏è  NEXT: Create '03_model_training.ipynb'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ NOTEBOOK 2 COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Data cleaned and preprocessed\")\n",
    "print(\"‚úÖ Categorical variables encoded\")\n",
    "print(\"‚úÖ Feature selection completed\")\n",
    "print(\"‚úÖ Train/test split created\")\n",
    "print(\"\\n‚û°Ô∏è  NEXT: Create '03_model_training.ipynb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafdc82-af56-4fc6-b2c3-587240c642e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
